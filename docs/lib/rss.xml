<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[notes]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib\media\favicon.png</url><title>notes</title><link/></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Thu, 07 Mar 2024 16:40:28 GMT</lastBuildDate><atom:link href="lib\rss.xml" rel="self" type="application/rss+xml"/><pubDate>Thu, 07 Mar 2024 16:40:25 GMT</pubDate><ttl>60</ttl><dc:creator/><item><title><![CDATA[Ground Truth <em>Kinda</em> Matters - About the In's and Out's of In-Context Learning]]></title><description><![CDATA[ 
 <br><br><br>With LLMs being all the rage currently, one big question is: how do we use them? Instead of small models trained to do a specific task, we now also have language models with billions of parameters that can be used for many different tasks. <br>One way to get a language model to do a certain task is...<br> In-context learning (ICL)
The basic idea behind in-context learning is to give a language model a few examples of inputs and corresponding outputs before letting it predict the outputs for new inputs.<br>
Intuitively, the examples demonstrate the task in hopes of the model generalizing from them to do the task (better).
<br>Now that you know what in-context learning is, we can dive into the main parts of this post. There are two parts, each of them breaking down one paper. Both try to answer the question of how and why in-context learning works. By the end, you hopefully have gained a better understanding of what goes into in-context learning and how the authors of the papers studied this.<br><br><br>The main paper this post focuses on is called "Rethinking the role of demonstrations: What makes in-context learning work?"<a href="about:blank#fn-1-7490379c2843d84f" class="footnote-link" target="_self" rel="noopener">[1]</a>. It came out in early 2022 and was one of the first to explore how and why in-context learning works.<br>Here's an overview showing you what to expect in this first part:<br>
<br><a class="internal-link" data-href="#Details about the experiments" href="about:blank#Details_about_the_experiments" target="_self" rel="noopener">Details about the experiments</a>
<br><a class="internal-link" data-href="#Gold labels vs random labels" href="about:blank#Gold_labels_vs_random_labels" target="_self" rel="noopener">Gold labels vs random labels</a>
<br><a class="internal-link" data-href="#What does not matter for ICL?" href="about:blank#What_does_not_matter_for_ICL" target="_self" rel="noopener">What does not matter for ICL?</a>
<br><a class="internal-link" data-href="#Ok, but ...what makes ICL work then?" href="about:blank#Ok,_but_...what_makes_ICL_work_then" target="_self" rel="noopener">What makes ICL work then?</a>
<br><a class="internal-link" data-href="#Recap and Discussion" href="about:blank#Recap_and_Discussion" target="_self" rel="noopener">Recap and Discussion</a>
<br><br><br>The authors used these 6 different LLMs in their experiments:<br><br>Each of this models was used with with two different inference methods, direct inference and noisy channel inference.<br>Direct vs Noisy Channel <a href="about:blank#fn-2-7490379c2843d84f" class="footnote-link" target="_self" rel="noopener">[2]</a>
The difference between these two inference methods lies in the calculated probabilities.
Direct models simply calculate , so we are trying to find the output with the largest probability given the input.
Noisy channel models take a slightly different approach: they try to maximize . Intuitively, the noisy channel method assumes that the input contains some noise, and that the goal is to reconstruct the version without noise - the output. This is more often used in areas where this intuition comes more naturally, like transcriptions or machine translations.
You may be able to see how these probabilities easily relate to each other via Bayes' rule. A nice advantage of the noisy channel method is that it splits the probability into two parts, and the second part, , can often be modelled independently from the task at hand. 
<br><br>The 26 datasets used in the experiments cover a wide variety of domains, with a focus on smaller datasets that pass the <a data-tooltip-position="top" aria-label="https://gluebenchmark.com/" rel="noopener" class="external-link" href="https://gluebenchmark.com/" target="_blank">GLUE</a> and <a data-tooltip-position="top" aria-label="https://super.gluebenchmark.com/" rel="noopener" class="external-link" href="https://super.gluebenchmark.com/" target="_blank">SuperGLUE</a> benchmarks. There are two types of tasks:<br>
<br>classification, e.g. sentiment analysis
<br>multi-choice, e.g. question answering
<br><br>The researchers arrived at a single combined score per model and task type with the following procedure:<br><br>First, they used 5 seeds to sample  examples from each dataset (default: ), and each set of  examples was concatenated. Then, each model was fed the example sets to make its predictions, one for each example set and inference method. The performance was then calculated as an average over the seeds and over the datasets.<br>If you're thinking: "That's a LOT of examples and runs", you'd be exactly right. That is why for the two largest models, they only used a subset of 6 datasets and 3 random seeds.<br>Now that we went over the setup, we are ready to dive into the different experiments and see what the results were!<br><br>The main experiments in the paper focused on comparing the following three approaches:<br>
<br>no demonstrations: a baseline without in-context learning, which means the model is not given any examples of input-output pairs
<br>demonstrations with gold labels: your usual in-context learning approach, in which the model is given k examples of inputs with their corresponding correct labels
<br>demonstrations with random labels: an approach where the model is given k examples just like in the previous approach, but the inputs are paired with randomly sampled labels
<br>This graph shows the performance of each model with each of the 3 approaches for each task category (classification and multi-choice):<br><img alt="Results of the main experiments, Figure 3 in the paper" src="fig3.png">Results of the main experiments, Figure 3 in the paper<br>It seems that using random labels in the examples works pretty much almost as well as using random labels! This is already the main point of this paper. The authors admit that this is unexpected, because - well, shouldn't the information that certain inputs produce certain outputs matter? Why else should we even give examples in the first place?<br>To try and answer these questions, the paper presents a whole bunch of further experiments. First, we will look at some smaller ablations of the main experiment, and then explore what else might be hidden in the examples that helps models learn in-context.<br><br>The following three experiments help investigate the main results. They were conducted on a smaller scale, using only the two best models (MetaICL and GPT-J) and 9 datasets (I'm sure the GPUs involved were grateful üôè).<br><br>One thing about the random labels method is that it is, well, random. In fact, the labels are sampled uniformly from all labels - which includes the correct ones. This means that there is a small chance that the some, or even most of the random labels are correct. Naturally, the comparison to the gold labels would not be as meaningful in that case.<br>The first ablation addresses this issue by using a range of fixed percentages of correct labels. <br><img alt="Results of experiments with varying percentages of correct labels, Figure 4 in the paper" src="fig4.png">Results of experiments with varying percentages of correct labels, Figure 4 in the paper<br>The main takeaway here is: using examples with all wrong labels still seems to produce noticeably better results than the no-example baseline. Also, the MetaICL model seems to be especially robust in this aspect.<br><br>As noted earlier, each experiment run used  examples. But especially with larger models, using long inputs can become vey costly . The second ablation therefore aims to see whether we really need that many examples to achieve the performance increases we've seen.<br>
<br><img alt="Results of experiments with varying numbers of demonstrations, Figure 5 in the paper" src="fig5.png">Results of experiments with varying numbers of demonstrations, Figure 5 in the paper<br>According to these results, increasing the number of examples beyond 8 does not really improve performance.<br><br>In the main experiments, the examples were simply concatenated without any additional context or structure. For the third ablation, the authors used what they describe as different templates for the examples.<br>Instead of just presenting the inputs and outputs, these templates are a way to add context and structure using short phrases or keywords that describe how inputs and outputs are related.<br>Here are some examples, taken from the paper's appendix, to show you the difference in how these templates present the inputs and outputs:<br>
What blocks sunshine? \n {summer|park|desktop|sea|moon}<br>
The question is: What blocks sunshine? \n The answer is: {summer|park|desktop|sea|moon}
<br>
Effect: I coughed. \n {Cause: I inhaled smoke.|Cause: I lowered my voice.}<br>
I coughed because {I inhaled smoke.|I lowered my voice.}
<br>As for the results: the use of special templates did not really make a difference in these experiments, and in some cases the performance even decreased:<br>
<img alt="Results of experiments with special templates, Figure 6 in the paper" src="fig6.png">Results of experiments with special templates, Figure 6 in the paper<br>Another paper<a href="about:blank#fn-3-7490379c2843d84f" class="footnote-link" target="_self" rel="noopener">[3]</a> that came out just this year actually focused on the question of what templates work best. They came to the conclusion that it really depends on the entire setup including the data, model, and examples - so it seems to be a very delicate and complex issue. If that is true, the experiment results here make a lot of sense, because they are averaged over examples and datasets.<br>So, according to these experiments, ICL seems to work even with fewer examples that have completely wrong labels and have little structure apart from simple concatenation of inputs and outputs. Which brings us to the question...<br><br>If the examples don't have the correct labels but still help improve performance, there must be something else that the models can extract from them. The authors of the paper identified four elements that examples in an ICL setting contain:<br>Information in ICL examples

<br>input-label mapping
<br>distribution of input text
<br>label space
<br>format

<br>The first one, input-label mapping, is the only aspect that had been removed in the experiments with the random/wrong labels. As for the other three, the paper presents - you guessed it! - more experiments to see which of them are actually needed to make ICL work.<br><br>First up is the distribution of input text, meaning: what typical inputs from the dataset look like.<br>
<br>
To see how important this is, the authors conducted experiments where they used example inputs from a different corpus (out-of-distribution demos) than the final input/question for which the model is asked to choose a label.<br><img alt="Results of experiments with out-of-distribution inputs, Figure 8 in the paper" src="fig8.png">Results of experiments with out-of-distribution inputs, Figure 8 in the paper<br>The out-of-distribution examples lead to noticeably weaker performance, with the exception of direct inference with the MetaICL model.  <br><br>Similarly to how the examples show typical inputs for a task, they also show typical labels. To study the impact of this, the next experiment used random English words instead of the actual labels. There are two levels of randomization here: the words are randomly sampled, assigned to the original labels and then randomly matched with inputs.<br><img alt="Results of experiments with random words as labels, Figure 9 in the paper" src="fig10.png">Results of experiments with random words as labels, Figure 9 in the paper<br>Apparently, there is a clear difference in trends between inference methods. For direct inference, there is a big performance drop, so having the correct label space represented in the examples seems to be very important for this type of inference. For the channel inference, the impact is much smaller. According to the authors, this is probably because this method only uses the labels for conditioning - so the model doesn't have to estimate the conditional probability of a label that it hasn't seen. <br><br>The most basic aspect of the examples is simply the fact that they are pairs of inputs and outputs. This is what is meant by the format. The variations used to look into whether this format is important for ICL performance were examples that left out either the inputs or the labels.<br>The aim here was to see how using only inputs or only labels in the demonstrations compares to using both components, with the only difference being the format itself. Things like the input distribution or the label space are additional information separate to the format itself, so the most interesting comparisons here - apart from the baseline without examples - are how using no inputs compares to using OOD inputs, and how using no labels compares to using random words as labels.<br><img alt="Results of experiments with incomplete format, Figure 10 in the paper" src="fig11.png">Results of experiments with incomplete format, Figure 10 in the paper<br>With both variants (no inputs and no labels), we see much weaker performance overall. Especially in the classification tasks (upper part of the figure), the format seems to make a large difference in settings where having OOD inputs or random words as labels achieved good performance. It looks like the format is very important!<br><br>Part 1: Main results

<br>specifying the input and label spaces is what's most important for better<br>
performance using ICL
<br>having only the right input distribution or label set still improves performance,<br>
as long as the format is right
<br>all of the trends were especially strong for the MetaICL model, which was fine-tuned<br>
for in-context learning

<br>These results have some pretty cool implications - for instance, if we only need the right input distribution and format to do in-context learning, then we can do it with unlabeled data too! <br>In a simpler (maybe also: more boring) world, this would be it - hooray, we don't need ground truth labels for in-context learning. But we obviously want to be diligent and honest when doing our research. That includes being aware and transparent about limitations. In this case, the authors mention the following:<br>
<br>they only used datasets with natural language inputs
<br>they macro-averaged over all datasets
<br>they focused on classification and multi-choice tasks only
<br>This brings us to...<br><br><br>The paper we've seen in the first part presented a result with quite bold implications - namely that using examples with correct labels isn't that important when doing in-context learning. There were also multiple limitations that the authors admitted - which brings us to the second paper I'd like to show you.<br>While doing my research on the first paper and browsing newer papers that cited it, this one in particular jumped out to me, because the title already made it clear that it is a direct response to the original paper: "Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations"<a href="about:blank#fn-4-7490379c2843d84f" class="footnote-link" target="_self" rel="noopener">[4]</a>.<br>
They mentioned two main limitations of the first paper:<br>
<br>the averaging across datasets
<br>the fact that apart from the general performance measures, there was no quantification of the impact that (not) using ground-truth labels had
<br>To address this, the authors of this second paper came up with two new metrics and replicated all the experiments in the first paper with an improved setup.<br>The two newly introduced measures are:<br>Label-correctness sensitivity
This measure expresses how much the performance changes with a certain amount of labels being corrupted - basically: the coefficient of the linear regression that relates the performance measure to the percentage of correctly labelled examples.
<br>Ground-truth label effect ratio (GLER)
This metric is best expressed as:<br>

<br><br>We've already looked at many experiments and many graphs, so I'm just going to quickly summarize the main results.<br>The average label-correctness sensitivity was 0.309 - which means that for each percentage of incorrect labels in examples, the performance measure dropped 0.309%. So for 100% incorrect labels, this result would imply that performance would drop about 30%!<br>
This number also varies a lot across datasets/tasks, so once again, there seem to be other factors at play. One of those factors seems to be task difficulty, which is also analyzed in the paper - in short, low sensitivity is apparently related to low performance in general. Additionally, noisy channel inference reduces sensitivity, while the number of examples and model size lead to an increase.<br>Finally, they also did some more ablations, with one being especially interesting:<br><br>If you think (or scroll üòé) back to the 4 aspects represented in ICL examples, there are two that involve the labels: the input-label mapping and the label space. There was one experiment in the first paper where the labels were replaced with random English words in such a way that not only was the set of labels a bunch of random words, but those new labels were also randomly assigned to inputs. But what if the original labels were mapped to random words and simply replaced without randomizing? In other words, what if the input-label mapping was preserved with other labels?<br>This is what the authors of the second paper tried to answer with an experiment using prior-free labels, meaning labels that don't contain any information about what they mean (e.g. "Positive", "Hate"). Their results indicate that using these kinds of labels does lead to better performance than random mappings, especially when using simple letters or numbers.<br><br>All in all:<br>Part 2: Main result
having the correct input-label mappings does make a performance difference, on some datasets more than on others
<br><br><br>Let's recap: input-label mappings do seem to matter for ICL demonstrations - but not always as much as one would think. Also: there is a lot more that goes into in-context learning than just showing a model how inputs relate to outputs. There's many more factors, both on the example level, like the input/label spaces and the format, and in terms of the broader setup, like the model size, model meta-training, dataset, or inference method. In a sense, it depends on the context.<br>The papers are two years old as this is being written, so there is already a lot of new research on how to do in-context learning and why it works. Here are some more interesting papers I found, if you are motivated to do more reading:<br>
<br><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2303.03846" rel="noopener" class="external-link" href="https://arxiv.org/abs/2303.03846" target="_blank">how larger models perform with incorrect or prior-free labels</a><a href="about:blank#fn-5-7490379c2843d84f" class="footnote-link" target="_self" rel="noopener">[5]</a>
<br><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2301.00234" rel="noopener" class="external-link" href="https://arxiv.org/abs/2301.00234" target="_blank">a survey about ICL in general from 2023</a><a href="about:blank#fn-6-7490379c2843d84f" class="footnote-link" target="_self" rel="noopener">[6]</a>
<br><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2401.11624" rel="noopener" class="external-link" href="https://arxiv.org/abs/2401.11624" target="_blank">a survey about using retrieved demonstrations in ICL, i.e. choosing specific demonstrations for an input</a><a href="about:blank#fn-7-7490379c2843d84f" class="footnote-link" target="_self" rel="noopener">[7]</a>
<br><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2305.01639" rel="noopener" class="external-link" href="https://arxiv.org/abs/2305.01639" target="_blank">a paper about privacy in ICL</a><a href="about:blank#fn-8-7490379c2843d84f" class="footnote-link" target="_self" rel="noopener">[8]</a>
<br>I hope you've learned a thing or two about in-context learning and understood the main ideas from the two papers.<br><br><br><br><br><br><br><br><br><br><br>
<br>
<br>Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., &amp; Zettlemoyer, L. (2022a). Rethinking the role of demonstrations: What makes in-context learning work?&nbsp;arXiv Preprint arXiv:2202.12837.&nbsp;<a rel="noopener" class="external-link" href="https://arxiv.org/abs/2202.12837" target="_blank">https://arxiv.org/abs/2202.12837</a><a href="about:blank#fnref-1-7490379c2843d84f" class="footnote-backref footnote-link" target="_self" rel="noopener">‚Ü©Ô∏é</a>
<br>Min, S., Lewis, M., Hajishirzi, H., &amp; Zettlemoyer, L. (2022). Noisy channel language model prompting for few-shot text classification.&nbsp;Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics Volume 1: Long Papers, 5316‚Äì5330.&nbsp;<a rel="noopener" class="external-link" href="https://aclanthology.org/2022.acl-long.365.pdf" target="_blank">https://aclanthology.org/2022.acl-long.365.pdf</a><a href="about:blank#fnref-2-7490379c2843d84f" class="footnote-backref footnote-link" target="_self" rel="noopener">‚Ü©Ô∏é</a>
<br>Voronov, A., Wolf, L., &amp; Ryabinin, M. (2024).&nbsp;Mind your format: Towards consistent evaluation of in-context learning improvements.&nbsp;<a rel="noopener" class="external-link" href="https://arxiv.org/abs/2401.06766" target="_blank">https://arxiv.org/abs/2401.06766</a><a href="about:blank#fnref-3-7490379c2843d84f" class="footnote-backref footnote-link" target="_self" rel="noopener">‚Ü©Ô∏é</a>
<br>Yoo, K. M., Kim, J., Kim, H. J., Cho, H., Jo, H., Lee, S.-W., Lee, S., &amp; Kim, T. (2022).&nbsp;Ground-truth labels matter: A deeper look into input-label demonstrations.&nbsp;<a rel="noopener" class="external-link" href="https://arxiv.org/abs/2205.12685" target="_blank">https://arxiv.org/abs/2205.12685</a><a href="about:blank#fnref-4-7490379c2843d84f" class="footnote-backref footnote-link" target="_self" rel="noopener">‚Ü©Ô∏é</a>
<br>Wei, J., Wei, J., Tay, Y., Tran, D., Webson, A., Lu, Y., Chen, X., Liu, H., Huang, D., Zhou, D., &amp; others. (2023). Larger language models do in-context learning differently.&nbsp;arXiv Preprint arXiv:2303.03846. <a rel="noopener" class="external-link" href="https://arxiv.org/abs/2303.03846" target="_blank">https://arxiv.org/abs/2303.03846</a><a href="about:blank#fnref-5-7490379c2843d84f" class="footnote-backref footnote-link" target="_self" rel="noopener">‚Ü©Ô∏é</a>
<br>Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu, J., Li, L., &amp; Sui, Z. (2023).&nbsp;A survey on in-context learning.&nbsp;<a rel="noopener" class="external-link" href="https://arxiv.org/abs/2301.00234" target="_blank">https://arxiv.org/abs/2301.00234</a><a href="about:blank#fnref-6-7490379c2843d84f" class="footnote-backref footnote-link" target="_self" rel="noopener">‚Ü©Ô∏é</a>
<br>Xu, X., Liu, Y., Pasupat, P., Kazemi, M., &amp; others. (2024). In-context learning with retrieved demonstrations for language models: A survey.&nbsp;arXiv Preprint arXiv:2401.11624. <a rel="noopener" class="external-link" href="https://arxiv.org/abs/2401.11624" target="_blank">https://arxiv.org/abs/2401.11624</a><a href="about:blank#fnref-7-7490379c2843d84f" class="footnote-backref footnote-link" target="_self" rel="noopener">‚Ü©Ô∏é</a>
<br>Panda, A., Wu, T., Wang, J. T., &amp; Mittal, P. (2023). Differentially private in-context learning.&nbsp;arXiv Preprint arXiv:2305.01639. <a rel="noopener" class="external-link" href="https://arxiv.org/abs/2305.01639" target="_blank">https://arxiv.org/abs/2305.01639</a><a href="about:blank#fnref-8-7490379c2843d84f" class="footnote-backref footnote-link" target="_self" rel="noopener">‚Ü©Ô∏é</a>
]]></description><link>index.html</link><guid isPermaLink="false">index.md</guid><pubDate>Thu, 07 Mar 2024 16:38:29 GMT</pubDate><enclosure url="fig3.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="fig3.png"&gt;&lt;/figure&gt;</content:encoded></item></channel></rss>